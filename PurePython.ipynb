{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ca4c59-5b3a-48d0-85f3-b7434d127d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Starting analysis for: Facebook Ads\n",
      "\n",
      "ðŸ“ˆ Overall Summary\n",
      "\n",
      "page_id:\n",
      "{'count': 246745, 'unique': 4475, 'most_common': ('4d66f5853f0365dba032a87704a634f023d15babde973bb7a284ed8cd2707b2d', 55503)}\n",
      "\n",
      "ad_id:\n",
      "{'count': 246745, 'unique': 246745, 'most_common': ('0ddb025b8544e2d58e6977ad417e742a52522b3e1fc1c9d9b61c57148f8d72fc', 1)}\n",
      "\n",
      "ad_creation_time:\n",
      "{'count': 246745, 'unique': 547, 'most_common': ('2024-10-27', 8619)}\n",
      "\n",
      "bylines:\n",
      "{'count': 245736, 'unique': 3790, 'most_common': ('HARRIS FOR PRESIDENT', 49788)}\n",
      "\n",
      "currency:\n",
      "{'count': 246745, 'unique': 18, 'most_common': ('USD', 246599)}\n",
      "\n",
      "delivery_by_region:\n",
      "{'count': 246745, 'unique': 141122, 'most_common': ('{}', 30989)}\n",
      "\n",
      "demographic_distribution:\n",
      "{'count': 246745, 'unique': 215622, 'most_common': ('{}', 30989)}\n",
      "\n",
      "estimated_audience_size:\n",
      "{'count': 246745, 'mean': 556462.8559687126, 'min': 0.0, 'max': 1000001.0, 'std_dev': 409863.9282788759}\n",
      "\n",
      "estimated_impressions:\n",
      "{'count': 246745, 'mean': 45601.52595189366, 'min': 499.0, 'max': 1000000.0, 'std_dev': 136790.49271020087}\n",
      "\n",
      "estimated_spend:\n",
      "{'count': 246745, 'mean': 1061.2914344768892, 'min': 49.0, 'max': 474999.0, 'std_dev': 4992.550631940592}\n",
      "\n",
      "publisher_platforms:\n",
      "{'count': 246745, 'unique': 9, 'most_common': (\"['facebook', 'instagram']\", 214434)}\n",
      "\n",
      "illuminating_scored_message:\n",
      "{'count': 246745, 'unique': 26338, 'most_common': ('e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855', 5972)}\n",
      "\n",
      "illuminating_mentions:\n",
      "{'count': 246745, 'unique': 278, 'most_common': ('[]', 73205)}\n",
      "\n",
      "scam_illuminating:\n",
      "{'count': 246745, 'mean': 0.07163265719670105, 'min': 0.0, 'max': 1.0, 'std_dev': 0.257878691674285}\n",
      "\n",
      "election_integrity_Truth_illuminating:\n",
      "{'count': 246745, 'mean': 0.05008814768283045, 'min': 0.0, 'max': 1.0, 'std_dev': 0.21812685562427525}\n",
      "\n",
      "advocacy_msg_type_illuminating:\n",
      "{'count': 246745, 'mean': 0.5486311779367363, 'min': 0.0, 'max': 1.0, 'std_dev': 0.4976293887347144}\n",
      "\n",
      "issue_msg_type_illuminating:\n",
      "{'count': 246745, 'mean': 0.38164907090315914, 'min': 0.0, 'max': 1.0, 'std_dev': 0.48579116663635885}\n",
      "\n",
      "attack_msg_type_illuminating:\n",
      "{'count': 246745, 'mean': 0.27185555938316885, 'min': 0.0, 'max': 1.0, 'std_dev': 0.44491585071295586}\n",
      "\n",
      "image_msg_type_illuminating:\n",
      "{'count': 246745, 'mean': 0.22270360088350322, 'min': 0.0, 'max': 1.0, 'std_dev': 0.4160609414941812}\n",
      "\n",
      "cta_msg_type_illuminating:\n",
      "{'count': 246745, 'mean': 0.5727694583476869, 'min': 0.0, 'max': 1.0, 'std_dev': 0.4946762637642767}\n",
      "\n",
      "engagement_cta_subtype_illuminating:\n",
      "{'count': 246745, 'mean': 0.12486980485926767, 'min': 0.0, 'max': 1.0, 'std_dev': 0.33057122786727233}\n",
      "\n",
      "fundraising_cta_subtype_illuminating:\n",
      "{'count': 246745, 'mean': 0.22848689943058623, 'min': 0.0, 'max': 1.0, 'std_dev': 0.4198578762143011}\n",
      "\n",
      "voting_cta_subtype_illuminating:\n",
      "{'count': 246745, 'mean': 0.1438448600782184, 'min': 0.0, 'max': 1.0, 'std_dev': 0.35093235289339764}\n",
      "\n",
      "covid_topic_illuminating:\n",
      "{'count': 246745, 'mean': 0.024875884009807697, 'min': 0.0, 'max': 1.0, 'std_dev': 0.15574682791164093}\n",
      "\n",
      "economy_topic_illuminating:\n",
      "{'count': 246745, 'mean': 0.12212202881517356, 'min': 0.0, 'max': 1.0, 'std_dev': 0.3274266923957781}\n",
      "\n",
      "education_topic_illuminating:\n",
      "{'count': 246745, 'mean': 0.01432653143934021, 'min': 0.0, 'max': 1.0, 'std_dev': 0.11883300019884126}\n",
      "\n",
      "environment_topic_illuminating:\n",
      "{'count': 246745, 'mean': 0.02124865752092241, 'min': 0.0, 'max': 1.0, 'std_dev': 0.14421217727529445}\n",
      "\n",
      "foreign_policy_topic_illuminating:\n",
      "{'count': 246745, 'mean': 0.005264544367667025, 'min': 0.0, 'max': 1.0, 'std_dev': 0.07236593770737645}\n",
      "\n",
      "governance_topic_illuminating:\n",
      "{'count': 246745, 'mean': 0.025641856977851627, 'min': 0.0, 'max': 1.0, 'std_dev': 0.15806439241201364}\n",
      "\n",
      "health_topic_illuminating:\n",
      "{'count': 246745, 'mean': 0.10918964923301384, 'min': 0.0, 'max': 1.0, 'std_dev': 0.31187701058812467}\n",
      "\n",
      "immigration_topic_illuminating:\n",
      "{'count': 246745, 'mean': 0.03356906928205232, 'min': 0.0, 'max': 1.0, 'std_dev': 0.18011714762783995}\n",
      "\n",
      "lgbtq_issues_topic_illuminating:\n",
      "{'count': 246745, 'mean': 0.0032300553202699142, 'min': 0.0, 'max': 1.0, 'std_dev': 0.05674171360558219}\n",
      "\n",
      "military_topic_illuminating:\n",
      "{'count': 246745, 'mean': 0.002176335893331172, 'min': 0.0, 'max': 1.0, 'std_dev': 0.04660042333939221}\n",
      "\n",
      "race_and_ethnicity_topic_illuminating:\n",
      "{'count': 246745, 'mean': 0.01243388923787716, 'min': 0.0, 'max': 1.0, 'std_dev': 0.1108119471731156}\n",
      "\n",
      "safety_topic_illuminating:\n",
      "{'count': 246745, 'mean': 0.03372307442906645, 'min': 0.0, 'max': 1.0, 'std_dev': 0.1805154527460685}\n",
      "\n",
      "social_and_cultural_topic_illuminating:\n",
      "{'count': 246745, 'mean': 0.10583801090194331, 'min': 0.0, 'max': 1.0, 'std_dev': 0.3076301778926499}\n",
      "\n",
      "technology_and_privacy_topic_illuminating:\n",
      "{'count': 246745, 'mean': 0.0011550386026059293, 'min': 0.0, 'max': 1.0, 'std_dev': 0.0339662257019001}\n",
      "\n",
      "womens_issue_topic_illuminating:\n",
      "{'count': 246745, 'mean': 0.08090944092078867, 'min': 0.0, 'max': 1.0, 'std_dev': 0.2726959906024914}\n",
      "\n",
      "incivility_illuminating:\n",
      "{'count': 246745, 'mean': 0.18752558309185596, 'min': 0.0, 'max': 1.0, 'std_dev': 0.39033285639043425}\n",
      "\n",
      "freefair_illuminating:\n",
      "{'count': 246745, 'mean': 0.006415530203246266, 'min': 0.0, 'max': 1.0, 'std_dev': 0.07983965916421175}\n",
      "\n",
      "fraud_illuminating:\n",
      "{'count': 246745, 'mean': 0.0026383513343735433, 'min': 0.0, 'max': 1.0, 'std_dev': 0.05129708019575728}\n",
      "\n",
      "ðŸ“‚ Grouped by page_id:\n",
      "\n",
      "Group: ('4ff23a48b53d988df50ddfebb0e442a984ab8f94e874ef9b9cb34394e0c5d230',)\n",
      "  estimated_audience_size: {'count': 33, 'mean': 43636.36363636364, 'min': 30000.0, 'max': 75000.0, 'std_dev': 20680.569392867867}\n",
      "  estimated_impressions: {'count': 33, 'mean': 66908.09090909091, 'min': 499.0, 'max': 374999.0, 'std_dev': 95130.56853015503}\n",
      "  estimated_spend: {'count': 33, 'mean': 467.1818181818182, 'min': 49.0, 'max': 2249.0, 'std_dev': 652.9441692474709}\n",
      "  scam_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  election_integrity_Truth_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  advocacy_msg_type_illuminating: {'count': 33, 'mean': 1.0, 'min': 1.0, 'max': 1.0, 'std_dev': 0.0}\n",
      "  issue_msg_type_illuminating: {'count': 33, 'mean': 1.0, 'min': 1.0, 'max': 1.0, 'std_dev': 0.0}\n",
      "  attack_msg_type_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  image_msg_type_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  cta_msg_type_illuminating: {'count': 33, 'mean': 0.42424242424242425, 'min': 0.0, 'max': 1.0, 'std_dev': 0.4942274675848512}\n",
      "  engagement_cta_subtype_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  fundraising_cta_subtype_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  voting_cta_subtype_illuminating: {'count': 33, 'mean': 0.42424242424242425, 'min': 0.0, 'max': 1.0, 'std_dev': 0.4942274675848512}\n",
      "  covid_topic_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  economy_topic_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  education_topic_illuminating: {'count': 33, 'mean': 0.42424242424242425, 'min': 0.0, 'max': 1.0, 'std_dev': 0.4942274675848512}\n",
      "  environment_topic_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  foreign_policy_topic_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  governance_topic_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  health_topic_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  immigration_topic_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  lgbtq_issues_topic_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  military_topic_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  race_and_ethnicity_topic_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  safety_topic_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  social_and_cultural_topic_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  technology_and_privacy_topic_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  womens_issue_topic_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  incivility_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  freefair_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  fraud_illuminating: {'count': 33, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "\n",
      "Group: ('b9eb7e353e596d5fc99568d4ef77d4b11ced3243537cbd0895dde3195b69b6be',)\n",
      "  estimated_audience_size: {'count': 3, 'mean': 1000001.0, 'min': 1000001.0, 'max': 1000001.0, 'std_dev': 0.0}\n",
      "  estimated_impressions: {'count': 3, 'mean': 16165.666666666666, 'min': 499.0, 'max': 47499.0, 'std_dev': 22156.01247717849}\n",
      "  estimated_spend: {'count': 3, 'mean': 82.33333333333333, 'min': 49.0, 'max': 149.0, 'std_dev': 47.14045207910317}\n",
      "  scam_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  election_integrity_Truth_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  advocacy_msg_type_illuminating: {'count': 3, 'mean': 0.3333333333333333, 'min': 0.0, 'max': 1.0, 'std_dev': 0.4714045207910317}\n",
      "  issue_msg_type_illuminating: {'count': 3, 'mean': 0.6666666666666666, 'min': 0.0, 'max': 1.0, 'std_dev': 0.4714045207910317}\n",
      "  attack_msg_type_illuminating: {'count': 3, 'mean': 0.6666666666666666, 'min': 0.0, 'max': 1.0, 'std_dev': 0.4714045207910317}\n",
      "  image_msg_type_illuminating: {'count': 3, 'mean': 0.3333333333333333, 'min': 0.0, 'max': 1.0, 'std_dev': 0.4714045207910317}\n",
      "  cta_msg_type_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  engagement_cta_subtype_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  fundraising_cta_subtype_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  voting_cta_subtype_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  covid_topic_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  economy_topic_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  education_topic_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  environment_topic_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  foreign_policy_topic_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  governance_topic_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  health_topic_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  immigration_topic_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  lgbtq_issues_topic_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  military_topic_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  race_and_ethnicity_topic_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  safety_topic_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  social_and_cultural_topic_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  technology_and_privacy_topic_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  womens_issue_topic_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  incivility_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  freefair_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  fraud_illuminating: {'count': 3, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "\n",
      "Group: ('7167146d80bba9d877a246d9682c7eecf3cae7b63337cf8ec01fff2eff27c909',)\n",
      "  estimated_audience_size: {'count': 7, 'mean': 1000001.0, 'min': 1000001.0, 'max': 1000001.0, 'std_dev': 0.0}\n",
      "  estimated_impressions: {'count': 7, 'mean': 641.8571428571429, 'min': 499.0, 'max': 1499.0, 'std_dev': 349.9271061118826}\n",
      "  estimated_spend: {'count': 7, 'mean': 49.0, 'min': 49.0, 'max': 49.0, 'std_dev': 0.0}\n",
      "  scam_illuminating: {'count': 7, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  election_integrity_Truth_illuminating: {'count': 7, 'mean': 0.14285714285714285, 'min': 0.0, 'max': 1.0, 'std_dev': 0.3499271061118826}\n",
      "  advocacy_msg_type_illuminating: {'count': 7, 'mean': 0.2857142857142857, 'min': 0.0, 'max': 1.0, 'std_dev': 0.45175395145262565}\n",
      "  issue_msg_type_illuminating: {'count': 7, 'mean': 0.2857142857142857, 'min': 0.0, 'max': 1.0, 'std_dev': 0.45175395145262565}\n",
      "  attack_msg_type_illuminating: {'count': 7, 'mean': 0.42857142857142855, 'min': 0.0, 'max': 1.0, 'std_dev': 0.4948716593053935}\n",
      "  image_msg_type_illuminating: {'count': 7, 'mean': 0.42857142857142855, 'min': 0.0, 'max': 1.0, 'std_dev': 0.4948716593053935}\n",
      "  cta_msg_type_illuminating: {'count': 7, 'mean': 0.14285714285714285, 'min': 0.0, 'max': 1.0, 'std_dev': 0.3499271061118826}\n",
      "  engagement_cta_subtype_illuminating: {'count': 7, 'mean': 0.14285714285714285, 'min': 0.0, 'max': 1.0, 'std_dev': 0.3499271061118826}\n",
      "  fundraising_cta_subtype_illuminating: {'count': 7, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  voting_cta_subtype_illuminating: {'count': 7, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  covid_topic_illuminating: {'count': 7, 'mean': 0.14285714285714285, 'min': 0.0, 'max': 1.0, 'std_dev': 0.3499271061118826}\n",
      "  economy_topic_illuminating: {'count': 7, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  education_topic_illuminating: {'count': 7, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  environment_topic_illuminating: {'count': 7, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  foreign_policy_topic_illuminating: {'count': 7, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  governance_topic_illuminating: {'count': 7, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  health_topic_illuminating: {'count': 7, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  immigration_topic_illuminating: {'count': 7, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  lgbtq_issues_topic_illuminating: {'count': 7, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  military_topic_illuminating: {'count': 7, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  race_and_ethnicity_topic_illuminating: {'count': 7, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  safety_topic_illuminating: {'count': 7, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  social_and_cultural_topic_illuminating: {'count': 7, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  technology_and_privacy_topic_illuminating: {'count': 7, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  womens_issue_topic_illuminating: {'count': 7, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  incivility_illuminating: {'count': 7, 'mean': 0.14285714285714285, 'min': 0.0, 'max': 1.0, 'std_dev': 0.3499271061118826}\n",
      "  freefair_illuminating: {'count': 7, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "  fraud_illuminating: {'count': 7, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0.0}\n",
      "\n",
      "ðŸ“‚ Grouped by page_id and ad_id:\n",
      "\n",
      "Group: ('4ff23a48b53d988df50ddfebb0e442a984ab8f94e874ef9b9cb34394e0c5d230', '0ddb025b8544e2d58e6977ad417e742a52522b3e1fc1c9d9b61c57148f8d72fc')\n",
      "  estimated_audience_size: {'count': 1, 'mean': 30000.0, 'min': 30000.0, 'max': 30000.0, 'std_dev': 0}\n",
      "  estimated_impressions: {'count': 1, 'mean': 47499.0, 'min': 47499.0, 'max': 47499.0, 'std_dev': 0}\n",
      "  estimated_spend: {'count': 1, 'mean': 249.0, 'min': 249.0, 'max': 249.0, 'std_dev': 0}\n",
      "  scam_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  election_integrity_Truth_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  advocacy_msg_type_illuminating: {'count': 1, 'mean': 1.0, 'min': 1.0, 'max': 1.0, 'std_dev': 0}\n",
      "  issue_msg_type_illuminating: {'count': 1, 'mean': 1.0, 'min': 1.0, 'max': 1.0, 'std_dev': 0}\n",
      "  attack_msg_type_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  image_msg_type_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  cta_msg_type_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  engagement_cta_subtype_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  fundraising_cta_subtype_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  voting_cta_subtype_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  covid_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  economy_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  education_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  environment_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  foreign_policy_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  governance_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  health_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  immigration_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  lgbtq_issues_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  military_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  race_and_ethnicity_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  safety_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  social_and_cultural_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  technology_and_privacy_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  womens_issue_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  incivility_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  freefair_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  fraud_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "\n",
      "Group: ('4ff23a48b53d988df50ddfebb0e442a984ab8f94e874ef9b9cb34394e0c5d230', '86229868e6bde3661724fe02da93504bb4fb5da8c2550d7b7cf193c687e89fa6')\n",
      "  estimated_audience_size: {'count': 1, 'mean': 75000.0, 'min': 75000.0, 'max': 75000.0, 'std_dev': 0}\n",
      "  estimated_impressions: {'count': 1, 'mean': 22499.0, 'min': 22499.0, 'max': 22499.0, 'std_dev': 0}\n",
      "  estimated_spend: {'count': 1, 'mean': 49.0, 'min': 49.0, 'max': 49.0, 'std_dev': 0}\n",
      "  scam_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  election_integrity_Truth_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  advocacy_msg_type_illuminating: {'count': 1, 'mean': 1.0, 'min': 1.0, 'max': 1.0, 'std_dev': 0}\n",
      "  issue_msg_type_illuminating: {'count': 1, 'mean': 1.0, 'min': 1.0, 'max': 1.0, 'std_dev': 0}\n",
      "  attack_msg_type_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  image_msg_type_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  cta_msg_type_illuminating: {'count': 1, 'mean': 1.0, 'min': 1.0, 'max': 1.0, 'std_dev': 0}\n",
      "  engagement_cta_subtype_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  fundraising_cta_subtype_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  voting_cta_subtype_illuminating: {'count': 1, 'mean': 1.0, 'min': 1.0, 'max': 1.0, 'std_dev': 0}\n",
      "  covid_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  economy_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  education_topic_illuminating: {'count': 1, 'mean': 1.0, 'min': 1.0, 'max': 1.0, 'std_dev': 0}\n",
      "  environment_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  foreign_policy_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  governance_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  health_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  immigration_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  lgbtq_issues_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  military_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  race_and_ethnicity_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  safety_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  social_and_cultural_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  technology_and_privacy_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  womens_issue_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  incivility_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  freefair_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  fraud_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "\n",
      "Group: ('4ff23a48b53d988df50ddfebb0e442a984ab8f94e874ef9b9cb34394e0c5d230', '07b5aefc27e872e971f793e49aac38496fa62e484f3928e2b6a2b6e3e08cac8d')\n",
      "  estimated_audience_size: {'count': 1, 'mean': 75000.0, 'min': 75000.0, 'max': 75000.0, 'std_dev': 0}\n",
      "  estimated_impressions: {'count': 1, 'mean': 32499.0, 'min': 32499.0, 'max': 32499.0, 'std_dev': 0}\n",
      "  estimated_spend: {'count': 1, 'mean': 149.0, 'min': 149.0, 'max': 149.0, 'std_dev': 0}\n",
      "  scam_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  election_integrity_Truth_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  advocacy_msg_type_illuminating: {'count': 1, 'mean': 1.0, 'min': 1.0, 'max': 1.0, 'std_dev': 0}\n",
      "  issue_msg_type_illuminating: {'count': 1, 'mean': 1.0, 'min': 1.0, 'max': 1.0, 'std_dev': 0}\n",
      "  attack_msg_type_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  image_msg_type_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  cta_msg_type_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  engagement_cta_subtype_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  fundraising_cta_subtype_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  voting_cta_subtype_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  covid_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  economy_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  education_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  environment_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  foreign_policy_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  governance_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  health_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  immigration_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  lgbtq_issues_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  military_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  race_and_ethnicity_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  safety_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  social_and_cultural_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  technology_and_privacy_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  womens_issue_topic_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  incivility_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  freefair_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "  fraud_illuminating: {'count': 1, 'mean': 0.0, 'min': 0.0, 'max': 0.0, 'std_dev': 0}\n",
      "\n",
      "ðŸ” Starting analysis for: Facebook Posts\n",
      "\n",
      "ðŸ“ˆ Overall Summary\n",
      "\n",
      "Facebook_Id:\n",
      "{'count': 19009, 'unique': 21, 'most_common': ('32fc18da91029ff09bf74fe9887eace6b5d2145809d583f696e344530508b064', 9013)}\n",
      "\n",
      "post_id:\n",
      "{'count': 19009, 'unique': 19009, 'most_common': ('8570b69695e00d8f06b12398ed525497e1712b5369c6fc2138fe98f69811c138', 1)}\n",
      "\n",
      "Page Category:\n",
      "{'count': 16537, 'unique': 6, 'most_common': ('PERSON', 9453)}\n",
      "\n",
      "Page Admin Top Country:\n",
      "{'count': 16280, 'unique': 1, 'most_common': ('US', 16280)}\n",
      "\n",
      "Post Created:\n",
      "{'count': 19009, 'unique': 18951, 'most_common': ('2023-11-14 11:11:44 EST', 2)}\n",
      "\n",
      "Post Created Date:\n",
      "{'count': 19009, 'unique': 425, 'most_common': ('2024-10-31', 103)}\n",
      "\n",
      "Post Created Time:\n",
      "{'count': 19009, 'unique': 16102, 'most_common': ('19:42:00', 7)}\n",
      "\n",
      "Type:\n",
      "{'count': 16544, 'unique': 9, 'most_common': ('Link', 7404)}\n",
      "\n",
      "Total Interactions:\n",
      "{'count': 19009, 'unique': 5665, 'most_common': ('23', 115)}\n",
      "\n",
      "Likes:\n",
      "{'count': 19009, 'mean': 2377.6954074385817, 'min': 0.0, 'max': 351979.0, 'std_dev': 11253.173660942848}\n",
      "\n",
      "Comments:\n",
      "{'count': 19009, 'mean': 901.583197432795, 'min': 0.0, 'max': 93872.0, 'std_dev': 3681.88324663013}\n",
      "\n",
      "Shares:\n",
      "{'count': 19009, 'mean': 320.53895523173236, 'min': 0.0, 'max': 76150.0, 'std_dev': 1722.114579655435}\n",
      "\n",
      "Love:\n",
      "{'count': 19009, 'mean': 413.87732126887266, 'min': 0.0, 'max': 244482.0, 'std_dev': 3730.842312335024}\n",
      "\n",
      "Wow:\n",
      "{'count': 19009, 'mean': 5.8683255300120996, 'min': 0.0, 'max': 4345.0, 'std_dev': 52.945585921667245}\n",
      "\n",
      "Haha:\n",
      "{'count': 19009, 'mean': 105.71971171550318, 'min': 0.0, 'max': 99276.0, 'std_dev': 942.0094692244772}\n",
      "\n",
      "Sad:\n",
      "{'count': 19009, 'mean': 10.172181598190331, 'min': 0.0, 'max': 56111.0, 'std_dev': 418.31400356540394}\n",
      "\n",
      "Angry:\n",
      "{'count': 19009, 'mean': 20.05550002630333, 'min': 0.0, 'max': 11814.0, 'std_dev': 156.01598493667757}\n",
      "\n",
      "Care:\n",
      "{'count': 19009, 'mean': 34.92903361565574, 'min': 0.0, 'max': 85236.0, 'std_dev': 790.0824307108217}\n",
      "\n",
      "Video Share Status:\n",
      "{'count': 3271, 'unique': 3, 'most_common': ('owned', 3100)}\n",
      "\n",
      "Is Video Owner?:\n",
      "{'count': 16544, 'unique': 3, 'most_common': ('-', 13280)}\n",
      "\n",
      "Post Views:\n",
      "{'count': 16544, 'mean': 6485.058510638298, 'min': 0.0, 'max': 4276477.0, 'std_dev': 90390.22650804598}\n",
      "\n",
      "Total Views:\n",
      "{'count': 16544, 'mean': 7461.847014023211, 'min': 0.0, 'max': 4462155.0, 'std_dev': 95973.37816684392}\n",
      "\n",
      "Total Views For All Crossposts:\n",
      "{'count': 16544, 'mean': 3555.9379835589943, 'min': 0.0, 'max': 4499458.0, 'std_dev': 88091.38586181619}\n",
      "\n",
      "Video Length:\n",
      "{'count': 3271, 'unique': 774, 'most_common': ('00:00:30', 83)}\n",
      "\n",
      "Sponsor Id:\n",
      "{'count': 0, 'mean': None, 'min': None, 'max': None, 'std_dev': None}\n",
      "\n",
      "Sponsor Name:\n",
      "{'count': 0, 'mean': None, 'min': None, 'max': None, 'std_dev': None}\n",
      "\n",
      "Sponsor Category:\n",
      "{'count': 0, 'mean': None, 'min': None, 'max': None, 'std_dev': None}\n",
      "\n",
      "Overperforming Score:\n",
      "{'count': 16544, 'mean': -2.744160420696325, 'min': -198.75, 'max': 246.78, 'std_dev': 7.8082409618565265}\n",
      "\n",
      "illuminating_scored_messageelection_integrity_Truth_illuminating:\n",
      "{'count': 0, 'mean': None, 'min': None, 'max': None, 'std_dev': None}\n",
      "\n",
      "advocacy_msg_type_illuminating:\n",
      "{'count': 19009, 'mean': 0.549266137092956, 'min': 0.0, 'max': 1.0, 'std_dev': 0.49756692789607515}\n",
      "\n",
      "issue_msg_type_illuminating:\n",
      "{'count': 19009, 'mean': 0.4603082750276185, 'min': 0.0, 'max': 1.0, 'std_dev': 0.49842207712812725}\n",
      "\n",
      "attack_msg_type_illuminating:\n",
      "{'count': 19009, 'mean': 0.2166868325530012, 'min': 0.0, 'max': 1.0, 'std_dev': 0.4119874380987227}\n",
      "\n",
      "image_msg_type_illuminating:\n",
      "{'count': 19009, 'mean': 0.14856120784891366, 'min': 0.0, 'max': 1.0, 'std_dev': 0.35565541662033706}\n",
      "\n",
      "cta_msg_type_illuminating:\n",
      "{'count': 19009, 'mean': 0.1328318165079699, 'min': 0.0, 'max': 1.0, 'std_dev': 0.3393928771072883}\n",
      "\n",
      "engagement_cta_subtype_illuminating:\n",
      "{'count': 19009, 'mean': 0.0908517018254511, 'min': 0.0, 'max': 1.0, 'std_dev': 0.2873981038574723}\n",
      "\n",
      "fundraising_cta_subtype_illuminating:\n",
      "{'count': 19009, 'mean': 0.01841233100110474, 'min': 0.0, 'max': 1.0, 'std_dev': 0.13443703756112188}\n",
      "\n",
      "voting_cta_subtype_illuminating:\n",
      "{'count': 19009, 'mean': 0.023409963701404597, 'min': 0.0, 'max': 1.0, 'std_dev': 0.15120164450462673}\n",
      "\n",
      "covid_topic_illuminating:\n",
      "{'count': 19009, 'mean': 0.05239623336314377, 'min': 0.0, 'max': 1.0, 'std_dev': 0.22282474748667114}\n",
      "\n",
      "economy_topic_illuminating:\n",
      "{'count': 19009, 'mean': 0.09037824188542269, 'min': 0.0, 'max': 1.0, 'std_dev': 0.2867228893533314}\n",
      "\n",
      "education_topic_illuminating:\n",
      "{'count': 19009, 'mean': 0.014992898100899574, 'min': 0.0, 'max': 1.0, 'std_dev': 0.12152411739007042}\n",
      "\n",
      "environment_topic_illuminating:\n",
      "{'count': 19009, 'mean': 0.022252617181335158, 'min': 0.0, 'max': 1.0, 'std_dev': 0.14750402777523094}\n",
      "\n",
      "foreign_policy_topic_illuminating:\n",
      "{'count': 19009, 'mean': 0.03687726866221264, 'min': 0.0, 'max': 1.0, 'std_dev': 0.1884604354187574}\n",
      "\n",
      "governance_topic_illuminating:\n",
      "{'count': 19009, 'mean': 0.03109053606186543, 'min': 0.0, 'max': 1.0, 'std_dev': 0.1735624228606275}\n",
      "\n",
      "health_topic_illuminating:\n",
      "{'count': 19009, 'mean': 0.04866116050291967, 'min': 0.0, 'max': 1.0, 'std_dev': 0.21515866694471955}\n",
      "\n",
      "immigration_topic_illuminating:\n",
      "{'count': 19009, 'mean': 0.04082276816244937, 'min': 0.0, 'max': 1.0, 'std_dev': 0.19787943238751288}\n",
      "\n",
      "lgbtq_issues_topic_illuminating:\n",
      "{'count': 19009, 'mean': 0.0033668262402020096, 'min': 0.0, 'max': 1.0, 'std_dev': 0.05792659770149026}\n",
      "\n",
      "military_topic_illuminating:\n",
      "{'count': 19009, 'mean': 0.0055763059603345785, 'min': 0.0, 'max': 1.0, 'std_dev': 0.07446617199891045}\n",
      "\n",
      "race_and_ethnicity_topic_illuminating:\n",
      "{'count': 19009, 'mean': 0.021568730601294123, 'min': 0.0, 'max': 1.0, 'std_dev': 0.14527050788629783}\n",
      "\n",
      "safety_topic_illuminating:\n",
      "{'count': 19009, 'mean': 0.032195275921931714, 'min': 0.0, 'max': 1.0, 'std_dev': 0.17651838468058334}\n",
      "\n",
      "social_and_cultural_topic_illuminating:\n",
      "{'count': 19009, 'mean': 0.06170761218370246, 'min': 0.0, 'max': 1.0, 'std_dev': 0.2406237369468944}\n",
      "\n",
      "technology_and_privacy_topic_illuminating:\n",
      "{'count': 19009, 'mean': 0.0020516597401230995, 'min': 0.0, 'max': 1.0, 'std_dev': 0.04524876166740762}\n",
      "\n",
      "womens_issue_topic_illuminating:\n",
      "{'count': 19009, 'mean': 0.0254616234415277, 'min': 0.0, 'max': 1.0, 'std_dev': 0.15752247196273153}\n",
      "\n",
      "incivility_illuminating:\n",
      "{'count': 19009, 'mean': 0.1278867904676732, 'min': 0.0, 'max': 1.0, 'std_dev': 0.33396370954274457}\n",
      "\n",
      "scam_illuminating:\n",
      "{'count': 18060, 'mean': 0.020210409745293468, 'min': 0.0, 'max': 1.0, 'std_dev': 0.1407193983899193}\n",
      "\n",
      "freefair_illuminating:\n",
      "{'count': 19009, 'mean': 0.0028407596401704457, 'min': 0.0, 'max': 1.0, 'std_dev': 0.053223018749759245}\n",
      "\n",
      "fraud_illuminating:\n",
      "{'count': 19009, 'mean': 0.00862749224051765, 'min': 0.0, 'max': 1.0, 'std_dev': 0.0924827476784587}\n",
      "\n",
      "ðŸ“‚ Grouped by page_id:\n",
      "\n",
      "ðŸ” Starting analysis for: Twitter Posts\n",
      "\n",
      "ðŸ“ˆ Overall Summary\n",
      "\n",
      "id:\n",
      "{'count': 27304, 'unique': 27304, 'most_common': ('cc46051622b8a9c1b883a3bbf12c640b12ac1cbdc7f48a773b6cc2a65f03aa2d', 1)}\n",
      "\n",
      "url:\n",
      "{'count': 27304, 'unique': 27304, 'most_common': ('f70a206472e9deaf6e313297c1efb891729ced346a0aeb34e16935d78f74b937', 1)}\n",
      "\n",
      "source:\n",
      "{'count': 27304, 'unique': 14, 'most_common': ('Twitter Web App', 14930)}\n",
      "\n",
      "retweetCount:\n",
      "{'count': 27304, 'mean': 1322.0551933782597, 'min': 0.0, 'max': 144615.0, 'std_dev': 3404.9418860146225}\n",
      "\n",
      "replyCount:\n",
      "{'count': 27304, 'mean': 1063.7850131848813, 'min': 0.0, 'max': 121270.0, 'std_dev': 3174.9235122787168}\n",
      "\n",
      "likeCount:\n",
      "{'count': 27304, 'mean': 6913.69282888954, 'min': 0.0, 'max': 915221.0, 'std_dev': 21589.912616632584}\n",
      "\n",
      "quoteCount:\n",
      "{'count': 27304, 'mean': 128.08156314093173, 'min': 0.0, 'max': 123320.0, 'std_dev': 1131.512746792056}\n",
      "\n",
      "viewCount:\n",
      "{'count': 27304, 'mean': 507084.7318341635, 'min': 5.0, 'max': 333502775.0, 'std_dev': 3212115.163351335}\n",
      "\n",
      "createdAt:\n",
      "{'count': 27304, 'unique': 27014, 'most_common': ('2023-10-06 04:55:21', 4)}\n",
      "\n",
      "lang:\n",
      "{'count': 27304, 'unique': 12, 'most_common': ('en', 27281)}\n",
      "\n",
      "bookmarkCount:\n",
      "{'count': 27304, 'mean': 136.21352182830356, 'min': 0.0, 'max': 42693.0, 'std_dev': 712.5672453544692}\n",
      "\n",
      "isReply:\n",
      "{'count': 27304, 'unique': 2, 'most_common': ('False', 23930)}\n",
      "\n",
      "isRetweet:\n",
      "{'count': 27304, 'unique': 1, 'most_common': ('False', 27304)}\n",
      "\n",
      "isQuote:\n",
      "{'count': 27304, 'unique': 2, 'most_common': ('False', 24064)}\n",
      "\n",
      "isConversationControlled:\n",
      "{'count': 27304, 'unique': 2, 'most_common': ('False', 27296)}\n",
      "\n",
      "quoteId:\n",
      "{'count': 3287, 'mean': 1.7642983966589504e+18, 'min': 7.912639390153769e+17, 'max': 1.8535761683325583e+18, 'std_dev': 6.8936377884959416e+16}\n",
      "\n",
      "inReplyToId:\n",
      "{'count': 3345, 'mean': 1.7582857247419999e+18, 'min': 1.2400673584227e+18, 'max': 1.853530653414859e+18, 'std_dev': 4.360544623583901e+16}\n",
      "\n",
      "month_year:\n",
      "{'count': 27304, 'unique': 15, 'most_common': ('2024-10', 3586)}\n",
      "\n",
      "illuminating_scored_message:\n",
      "{'count': 27304, 'unique': 27136, 'most_common': ('36cb7d55fcf85362ca03f624c2f574f1f55f89db559b17da084df6e643afe5cd', 21)}\n",
      "\n",
      "election_integrity_Truth_illuminating:\n",
      "{'count': 26034, 'mean': 0.03714373511561804, 'min': 0.0, 'max': 1.0, 'std_dev': 0.1891139287764887}\n",
      "\n",
      "advocacy_msg_type_illuminating:\n",
      "{'count': 26034, 'mean': 0.5636859491434278, 'min': 0.0, 'max': 1.0, 'std_dev': 0.495927514745553}\n",
      "\n",
      "issue_msg_type_illuminating:\n",
      "{'count': 26034, 'mean': 0.507682261657832, 'min': 0.0, 'max': 1.0, 'std_dev': 0.49994097937238613}\n",
      "\n",
      "attack_msg_type_illuminating:\n",
      "{'count': 26034, 'mean': 0.3075977567795959, 'min': 0.0, 'max': 1.0, 'std_dev': 0.4614990539575964}\n",
      "\n",
      "image_msg_type_illuminating:\n",
      "{'count': 26034, 'mean': 0.22643466236460014, 'min': 0.0, 'max': 1.0, 'std_dev': 0.4185236027327846}\n",
      "\n",
      "cta_msg_type_illuminating:\n",
      "{'count': 26034, 'mean': 0.10966428516555274, 'min': 0.0, 'max': 1.0, 'std_dev': 0.31247084619957916}\n",
      "\n",
      "engagement_cta_subtype_illuminating:\n",
      "{'count': 26034, 'mean': 0.0669124990397173, 'min': 0.0, 'max': 1.0, 'std_dev': 0.24987039943133946}\n",
      "\n",
      "fundraising_cta_subtype_illuminating:\n",
      "{'count': 26034, 'mean': 0.007874318199277867, 'min': 0.0, 'max': 1.0, 'std_dev': 0.08838729157618978}\n",
      "\n",
      "voting_cta_subtype_illuminating:\n",
      "{'count': 26034, 'mean': 0.016785741722363065, 'min': 0.0, 'max': 1.0, 'std_dev': 0.12846781930582143}\n",
      "\n",
      "covid_topic_illuminating:\n",
      "{'count': 26034, 'mean': 0.0076054390412537455, 'min': 0.0, 'max': 1.0, 'std_dev': 0.08687690336472358}\n",
      "\n",
      "economy_topic_illuminating:\n",
      "{'count': 26034, 'mean': 0.16021356687408772, 'min': 0.0, 'max': 1.0, 'std_dev': 0.3668040074257231}\n",
      "\n",
      "education_topic_illuminating:\n",
      "{'count': 26034, 'mean': 0.01843742797879696, 'min': 0.0, 'max': 1.0, 'std_dev': 0.13452690893766805}\n",
      "\n",
      "environment_topic_illuminating:\n",
      "{'count': 26034, 'mean': 0.028539602058846123, 'min': 0.0, 'max': 1.0, 'std_dev': 0.1665085378386611}\n",
      "\n",
      "foreign_policy_topic_illuminating:\n",
      "{'count': 26034, 'mean': 0.042252439118076364, 'min': 0.0, 'max': 1.0, 'std_dev': 0.2011645359069277}\n",
      "\n",
      "governance_topic_illuminating:\n",
      "{'count': 26034, 'mean': 0.022969962356917877, 'min': 0.0, 'max': 1.0, 'std_dev': 0.14980768734026853}\n",
      "\n",
      "health_topic_illuminating:\n",
      "{'count': 26034, 'mean': 0.05565798571099332, 'min': 0.0, 'max': 1.0, 'std_dev': 0.2292600583127994}\n",
      "\n",
      "immigration_topic_illuminating:\n",
      "{'count': 26034, 'mean': 0.06529922409157256, 'min': 0.0, 'max': 1.0, 'std_dev': 0.2470531024387493}\n",
      "\n",
      "lgbtq_issues_topic_illuminating:\n",
      "{'count': 26034, 'mean': 0.0030729046631328264, 'min': 0.0, 'max': 1.0, 'std_dev': 0.05534854939439807}\n",
      "\n",
      "military_topic_illuminating:\n",
      "{'count': 26034, 'mean': 0.010985634170699855, 'min': 0.0, 'max': 1.0, 'std_dev': 0.10423507093376684}\n",
      "\n",
      "race_and_ethnicity_topic_illuminating:\n",
      "{'count': 26034, 'mean': 0.015402934623953292, 'min': 0.0, 'max': 1.0, 'std_dev': 0.1231490325943469}\n",
      "\n",
      "safety_topic_illuminating:\n",
      "{'count': 26034, 'mean': 0.03760467081508796, 'min': 0.0, 'max': 1.0, 'std_dev': 0.19023816532961213}\n",
      "\n",
      "social_and_cultural_topic_illuminating:\n",
      "{'count': 26034, 'mean': 0.05197050011523392, 'min': 0.0, 'max': 1.0, 'std_dev': 0.22196749138783}\n",
      "\n",
      "technology_and_privacy_topic_illuminating:\n",
      "{'count': 26034, 'mean': 0.0020357993393254974, 'min': 0.0, 'max': 1.0, 'std_dev': 0.04507388224210889}\n",
      "\n",
      "womens_issue_topic_illuminating:\n",
      "{'count': 26034, 'mean': 0.02331566413152032, 'min': 0.0, 'max': 1.0, 'std_dev': 0.1509040885384702}\n",
      "\n",
      "incivility_illuminating:\n",
      "{'count': 26034, 'mean': 0.17857417223630637, 'min': 0.0, 'max': 1.0, 'std_dev': 0.3829953488574298}\n",
      "\n",
      "scam_illuminating:\n",
      "{'count': 26034, 'mean': 0.012368441269109626, 'min': 0.0, 'max': 1.0, 'std_dev': 0.1105235854000503}\n",
      "\n",
      "freefair_illuminating:\n",
      "{'count': 27304, 'mean': 0.0014283621447406974, 'min': 0.0, 'max': 1.0, 'std_dev': 0.03776667745942405}\n",
      "\n",
      "fraud_illuminating:\n",
      "{'count': 27304, 'mean': 0.0027468502783474947, 'min': 0.0, 'max': 1.0, 'std_dev': 0.05233837112383072}\n",
      "\n",
      "ðŸ“‚ Grouped by page_id:\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Downloads/period_03/trump_truths_dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 84\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Optional: Extra dataset (Trump Truths)\u001b[39;00m\n\u001b[0;32m     83\u001b[0m extra_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloads/period_03/trump_truths_dataset.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 84\u001b[0m extra_data \u001b[38;5;241m=\u001b[39m \u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mextra_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m num_cols \u001b[38;5;241m=\u001b[39m identify_numeric_columns(extra_data)\n\u001b[0;32m     86\u001b[0m summary \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-inf\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m: []})\n",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_csv\u001b[39m(file_path):\n\u001b[1;32m----> 7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      8\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(csv\u001b[38;5;241m.\u001b[39mDictReader(file))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Downloads/period_03/trump_truths_dataset.csv'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "def read_csv(file_path):\n",
    "    with open(file_path, encoding='utf-8') as file:\n",
    "        return list(csv.DictReader(file))\n",
    "\n",
    "def is_numeric(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def identify_numeric_columns(data):\n",
    "    return [\n",
    "        col for col in data[0]\n",
    "        if all(is_numeric(row[col]) or row[col] == '' for row in data[:100])\n",
    "    ]\n",
    "\n",
    "def get_numeric_stats(data, col):\n",
    "    numbers = [float(row[col]) for row in data if is_numeric(row[col])]\n",
    "    if not numbers:\n",
    "        return {\"count\": 0, \"mean\": None, \"min\": None, \"max\": None, \"std_dev\": None}\n",
    "    avg = sum(numbers) / len(numbers)\n",
    "    std = math.sqrt(sum((x - avg) ** 2 for x in numbers) / len(numbers)) if len(numbers) > 1 else 0\n",
    "    return {\"count\": len(numbers), \"mean\": avg, \"min\": min(numbers), \"max\": max(numbers), \"std_dev\": std}\n",
    "\n",
    "def get_categorical_stats(data, col):\n",
    "    values = [row[col] for row in data if row[col]]\n",
    "    freq = Counter(values)\n",
    "    return {\"count\": len(values), \"unique\": len(freq), \"most_common\": freq.most_common(1)[0] if freq else None}\n",
    "\n",
    "def group_data(data, keys):\n",
    "    result = defaultdict(list)\n",
    "    for row in data:\n",
    "        if all(k in row for k in keys):\n",
    "            result[tuple(row[k] for k in keys)].append(row)\n",
    "    return result\n",
    "\n",
    "def analyze_file(file_path, label):\n",
    "    print(f\"\\nðŸ” Starting analysis for: {label}\")\n",
    "    data = read_csv(file_path)\n",
    "    numeric_cols = identify_numeric_columns(data)\n",
    "\n",
    "    print(\"\\nðŸ“ˆ Overall Summary\")\n",
    "    for col in data[0]:\n",
    "        print(f\"\\n{col}:\")\n",
    "        if col in numeric_cols:\n",
    "            print(get_numeric_stats(data, col))\n",
    "        else:\n",
    "            print(get_categorical_stats(data, col))\n",
    "\n",
    "    print(\"\\nðŸ“‚ Grouped by page_id:\")\n",
    "    for group_key, rows in list(group_data(data, [\"page_id\"]).items())[:3]:\n",
    "        print(f\"\\nGroup: {group_key}\")\n",
    "        for col in numeric_cols:\n",
    "            print(f\"  {col}:\", get_numeric_stats(rows, col))\n",
    "\n",
    "    if \"ad_id\" in data[0]:\n",
    "        print(\"\\nðŸ“‚ Grouped by page_id and ad_id:\")\n",
    "        for group_key, rows in list(group_data(data, [\"page_id\", \"ad_id\"]).items())[:3]:\n",
    "            print(f\"\\nGroup: {group_key}\")\n",
    "            for col in numeric_cols:\n",
    "                print(f\"  {col}:\", get_numeric_stats(rows, col))\n",
    "\n",
    "# Time tracker\n",
    "start = time.time()\n",
    "\n",
    "# Process each dataset\n",
    "datasets = [\n",
    "    (\"2024_fb_ads_president_scored_anon.csv\", \"Facebook Ads\"),\n",
    "    (\"2024_fb_posts_president_scored_anon.csv\", \"Facebook Posts\"),\n",
    "    (\"2024_tw_posts_president_scored_anon.csv\", \"Twitter Posts\"),\n",
    "]\n",
    "\n",
    "for path, name in datasets:\n",
    "    analyze_file(path, name)\n",
    "\n",
    "# Optional: Extra dataset (Trump Truths)\n",
    "extra_path = \"Downloads/period_03/trump_truths_dataset.csv\"\n",
    "extra_data = read_csv(extra_path)\n",
    "num_cols = identify_numeric_columns(extra_data)\n",
    "summary = defaultdict(lambda: {\"count\": 0, \"sum\": 0, \"min\": float('inf'), \"max\": float('-inf'), \"values\": []})\n",
    "\n",
    "for row in extra_data:\n",
    "    for col, val in row.items():\n",
    "        if val == '':\n",
    "            continue\n",
    "        summary[col][\"count\"] += 1\n",
    "        if col in num_cols:\n",
    "            val = float(val)\n",
    "            summary[col][\"sum\"] += val\n",
    "            summary[col][\"min\"] = min(summary[col][\"min\"], val)\n",
    "            summary[col][\"max\"] = max(summary[col][\"max\"], val)\n",
    "            summary[col][\"values\"].append(val)\n",
    "        else:\n",
    "            summary[col][\"values\"].append(val)\n",
    "\n",
    "print(\"\\nðŸ“˜ Summary: Trump Truths Dataset\")\n",
    "for col, s in summary.items():\n",
    "    print(f\"\\n{col}\")\n",
    "    print(f\" - Count: {s['count']}\")\n",
    "    if col in num_cols:\n",
    "        mean = s['sum'] / s['count']\n",
    "        std = math.sqrt(sum((x - mean) ** 2 for x in s['values']) / s['count'])\n",
    "        print(f\" - Mean: {mean:.2f}, Min: {s['min']}, Max: {s['max']}, Std Dev: {std:.2f}\")\n",
    "    else:\n",
    "        freq = Counter(s['values']).most_common(3)\n",
    "        print(f\" - Unique: {len(set(s['values']))}, Top 3: {freq}\")\n",
    "\n",
    "print(f\"\\nâ±ï¸ Total Time: {time.time() - start:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a45ed8e-b4f9-43b9-a949-56de39a69a41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
